<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yizhuo Wang </title> <meta name="author" content="Yizhuo Wang"> <meta name="description" content="Yizhuo Wang's Homepage. "> <meta name="keywords" content="jekyll, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?0b8cc9054715060a603627ee24921d5e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yizhuo-wang.com/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/YizhuoCV.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl+K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yizhuo</span> Wang </h1> <p class="desc">Ph.D. candidate at <a href="https://www.nus.edu.sg/" rel="external nofollow noopener" target="_blank">National University of Singapore</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?8806fc02090327bd015b5558863fb0c2" class="img-fluid z-depth-1 rounded smaller-image" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am currently a Ph.D. student at the <a href="https://cde.nus.edu.sg/me/" rel="external nofollow noopener" target="_blank">National University of Singapore (NUS)</a>. I research in the <a href="https://marmotlab.org/" rel="external nofollow noopener" target="_blank">Multi-Agent Robotic Motion (MARMot) Laboratory</a> advised by Asst. Prof. <a href="https://cde.nus.edu.sg/me/staff/sartoretti-guillaume-a/" rel="external nofollow noopener" target="_blank">Guillaume Sartoretti</a>. Before this, I obtained my Master of Science degree from NUS and a B.Eng degree in Mechatronics from the <a href="http://en.hit.edu.cn/" rel="external nofollow noopener" target="_blank">Harbin Institute of Technology (HIT)</a>.</p> <p>My primary research interests lie in the areas of robot learning, coordinated path planning, and multi-agent reinforcement learning. In the long term, I aim to enhance the quality, scalability, and generalizability of decision-making in multi-agent systems, to accelerate our progress toward a future world increasingly populated with intelligent agents.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 05, 2024</th> <td> Our paper <a href="https://openreview.net/pdf?id=EPujQZWemk" rel="external nofollow noopener" target="_blank">ViPER</a>, on visibility-based pursuit-evasion leveraging a graph attention network for multi-agent coordination via reinforcement learning, has been accepted at <a href="https://www.corl.org/" rel="external nofollow noopener" target="_blank">Conference on Robot Learning (CoRL 2024)</a>! ðŸ¤– </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 10, 2024</th> <td> I pass my oral qualifying exam and become a Ph.D. candidate. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 19, 2024</th> <td> I create my personal website.ðŸŽ‰ </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoRLâ€™24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/viper_preview-480.webp 480w,/assets/img/publication_preview/viper_preview-800.webp 800w,/assets/img/publication_preview/viper_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/viper_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="viper_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2024viper" class="col-sm-8"> <div class="title">ViPER: Visibility-based Pursuit-Evasion via Reinforcement Learning</div> <div class="author"> <em>Yizhuo Wang</em>,Â <a href="https://www.yuhongcao.online/" rel="external nofollow noopener" target="_blank">Yuhong Cao</a>,Â <a href="https://www.linkedin.com/in/jimmychiun/" rel="external nofollow noopener" target="_blank">Jimmy Chiun</a>,Â Subhadeep Koley,Â Mandy Pham,Â andÂ Guillaume Sartoretti </div> <div class="periodical"> <em>In Conference on Robot Learning</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=EPujQZWemk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=EPujQZWemk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/marmotlab/ViPER" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In visibility-based pursuit-evasion tasks, a team of mobile pursuer robots with limited sensing capabilities is tasked with detecting all evaders in a multiply-connected planar environment, whose map may or may not be known to pursuers beforehand. This requires tight coordination among multiple agents to ensure that the omniscient and potentially arbitrarily fast evaders are guaranteed to be detected by the pursuers. Whereas existing methods typically rely on a relatively large team of agents to clear the environment, we propose ViPER, a neural solution that leverages a graph attention network to learn a coordinated yet distributed policy via multi-agent reinforcement learning (MARL). We experimentally demonstrate that ViPER significantly outperforms other state-of-the-art non-learning planners, showcasing its emergent coordinated behaviors and adaptability to more challenging scenarios and various team sizes, and finally deploy its learned policies on hardware in an aerial search task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROSâ€™23</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/stamp_preview-480.webp 480w,/assets/img/publication_preview/stamp_preview-800.webp 800w,/assets/img/publication_preview/stamp_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/stamp_preview.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="stamp_preview.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2023spatio" class="col-sm-8"> <div class="title">Spatio-Temporal Attention Network for Persistent Monitoring of Multiple Mobile Targets</div> <div class="author"> <em>Yizhuo Wang</em>,Â <a href="https://wyt2019suzhou.github.io/" rel="external nofollow noopener" target="_blank">Yutong Wang</a>,Â <a href="https://www.yuhongcao.online/" rel="external nofollow noopener" target="_blank">Yuhong Cao</a>,Â andÂ Guillaume Sartoretti </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10341674" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2303.06350" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/marmotlab/STAMP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This work focuses on the persistent monitoring problem, where a set of targets moving based on an unknown model must be monitored by an autonomous mobile robot with a limited sensing range. To keep each targetâ€™s position estimate as accurate as possible, the robot needs to adaptively plan its path to (re-)visit all the targets and update its belief from measurements collected along the way. In doing so, the main challenge is to strike a balance between exploitation, i.e., re-visiting previously-located targets, and exploration, i.e., finding new targets or re-acquiring lost ones. Encouraged by recent advances in deep reinforcement learning, we introduce an attention-based neural solution to the persistent monitoring problem, where the agent can learn the inter-dependencies between targets, i.e., their spatial and temporal correlations, conditioned on past measurements. This endows the agent with the ability to determine which target, time, and location to attend to across multiple scales, which we show also helps relax the usual limitations of a finite target set. We experimentally demonstrate that our method outperforms other baselines in terms of number of targets visits and average estimation error in complex environments. Finally, we implement and validate our model in a drone-based simulation experiment to monitor mobile ground targets in a high-fidelity simulator.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoRLâ€™22</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/catnipp_sim_preview-480.webp 480w,/assets/img/publication_preview/catnipp_sim_preview-800.webp 800w,/assets/img/publication_preview/catnipp_sim_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/catnipp_sim_preview.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="catnipp_sim_preview.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="cao2022catnipp" class="col-sm-8"> <div class="title">CAtNIPP: Context-Aware Attention-based Network for Informative Path Planning</div> <div class="author"> <a href="https://www.yuhongcao.online/" rel="external nofollow noopener" target="_blank">Yuhong Cao</a> ,Â <em>Yizhuo Wang</em>,Â Apoorva Vashisth,Â Haolin Fan,Â andÂ Guillaume Adrien Sartoretti </div> <div class="periodical"> <em>In Conference on Robot Learning</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=cAIIbdNAeNa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=cAIIbdNAeNa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/marmotlab/CAtNIPP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Informative path planning (IPP) is an NP-hard problem, which aims at planning a path allowing an agent to build an accurate belief about a quantity of interest throughout a given search domain, within constraints on resource budget (e.g., path length for robots with limited battery life). IPP requires frequent online replanning as this belief is updated with every new measurement (i.e., adaptive IPP), while balancing short-term exploitation and longer-term exploration to avoid suboptimal, myopic behaviors. Encouraged by the recent developments in deep reinforcement learning, we introduce CAtNIPP, a fully reactive, neural approach to the adaptive IPP problem. CAtNIPP relies on self-attention for its powerful ability to capture dependencies in data at multiple spatial scales. Specifically, our agent learns to form a context of its belief over the entire domain, which it uses to sequence local movement decisions that optimize short- and longer-term search objectives. We experimentally demonstrate that CAtNIPP significantly outperforms state-of-the-art non-learning IPP solvers in terms of solution quality and computing time once trained, and present experimental results on hardware.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">J-AAMAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/smac_preview-480.webp 480w,/assets/img/publication_preview/smac_preview-800.webp 800w,/assets/img/publication_preview/smac_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/smac_preview.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="smac_preview.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2023full" class="col-sm-8"> <div class="title">Full Communication Memory Networks for Team-Level Cooperation Learning</div> <div class="author"> <a href="https://wyt2019suzhou.github.io/" rel="external nofollow noopener" target="_blank">Yutong Wang</a> ,Â <em>Yizhuo Wang</em>,Â andÂ Guillaume Sartoretti </div> <div class="periodical"> <em>Autonomous Agents and Multi-Agent Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s10458-023-09617-6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://assets-eu.researchsquare.com/files/rs-2563058/v1_covered_5f11203c-27d3-4d75-8398-59fc4575fa38.pdf?c=1697495290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/marmotlab/FCMNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Communication in multi-agent systems is a key driver of team-level cooperation, for instance allowing individual agents to augment their knowledge about the world in partially-observable environments. In this paper, we propose two reinforcement learning-based multi-agent models, namely FCMNet and FCMTran. The two models both allow agents to simultaneously learn a differentiable communication mechanism that connects all agents as well as a common, cooperative policy conditioned upon received information. FCMNet utilizes multiple directional Long Short-Term Memory chains to sequentially transmit and encode the current observation-based messages sent by every other agent at each timestep. FCMTran further relies on the encoder of a modified transformer to simultaneously aggregate multiple self-generated messages sent by all agents at the previous timestep into a single message that is used in the current timestep. Results from evaluating our models on a challenging set of StarCraft II micromanagement tasks with shared rewards show that FCMNet and FCMTran both outperform recent communication-based methods and value decomposition methods in almost all tested StarCraft II micromanagement tasks. We further improve the performance of our models by combining them with value decomposition techniques; there, in particular, we show that FCMTran with value decomposition significantly pushes the state-of-the-art on one of the hardest benchmark tasks without any task-specific tuning. We also investigate the robustness of FCMNet under communication disturbances (i.e., binarized messages, random message loss, and random communication order) in an asymmetric collaborative pathfinding task with individual rewards, demonstrating FMCNetâ€™s potential applicability in real-world robotic tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRAâ€™23 Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gazectrl_preview-480.webp 480w,/assets/img/publication_preview/gazectrl_preview-800.webp 800w,/assets/img/publication_preview/gazectrl_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/gazectrl_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gazectrl_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2023learning" class="col-sm-8"> <div class="title">Learning Simultaneous Motion Planning and Active Gaze Control for Persistent Monitoring of Dynamic Targets</div> <div class="author"> <em>Yizhuo Wang</em>,Â andÂ Guillaume Sartoretti </div> <div class="periodical"> <em>In ICRA 2023 Workshop on Active Methods in Autonomous Navigation</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://robotics.pme.duth.gr/workshop_active2/wp-content/uploads/2023/05/11.-Motion_gaze_control.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We consider the persistent monitoring problem of a given set of targets moving based on an unknown model by an autonomous mobile robot equipped with a directional sensor (e.g., camera). The robot needs to actively plan both its path and its sensorâ€™s gaze/heading direction to detect and constantly re-locate all targets by collecting measurements along its path, to keep the estimated position of each target as accurate as possible at all times. Our recent work discretize the monitoring domain into a graph where the deep-reinforcement-learning-based agent sequentially decide which neighboring node to visit next. In this work, we extend it by duplicating neighboring features multiple times and then combining with its unique gaze features to output a joint decision of â€œwhere to go" and â€œwhere to look". Our simulation experiments show that active gaze control enhances monitoring performance, particularly in terms of minimum number of re-observation per target, compared to agent with a fixed forward-gaze sensor or greedy gaze selection.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%77%79%39%38@%75.%6E%75%73.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wyzh98" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yizhuo-wang-3baa9615b" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note">Reach out to me on any of the platforms listed here! </div> </div> </article> </div> <style>.smaller-image{max-width:200px}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Yizhuo Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> and <a href="https://azure.microsoft.com/" rel="external nofollow noopener" target="_blank">MicrosoftÂ® Azure. </a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-HKE1XBCZ70"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-HKE1XBCZ70");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/YizhuoCV.pdf"}},{id:"news-our-paper-lt-a-href-quot-https-openreview-net-pdf-id-epujqzwemk-quot-gt-viper-lt-a-gt-on-visibility-based-pursuit-evasion-leveraging-a-graph-attention-network-for-multi-agent-coordination-via-reinforcement-learning-has-been-accepted-at-lt-a-href-quot-https-www-corl-org-quot-gt-conference-on-robot-learning-corl-2024-lt-a-gt",title:"Our paper &lt;a href=&quot;https://openreview.net/pdf?id=EPujQZWemk&quot;&gt;ViPER&lt;/a&gt;, on visibility-based pursuit-evasion leveraging a graph attention network for multi-agent coordination via reinforcement learning, has been accepted at &lt;a href=&quot;https://www.corl.org/&quot;&gt;Conference on Robot Learning (CoRL 2024)&lt;/a&gt;! \ud83e\udd16",description:"",section:"News"},{id:"news-i-pass-my-oral-qualifying-exam-and-become-a-ph-d-candidate",title:"I pass my oral qualifying exam and become a Ph.D. candidate.",description:"",section:"News"},{id:"news-i-create-my-personal-website",title:"I create my personal website.\ud83c\udf89",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%79%39%38@%75.%6E%75%73.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wyzh98","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/yizhuo-wang-3baa9615b","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>